{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "def unpickle(file):\n",
    "    with open(file, 'rb') as f:\n",
    "        dict = pickle.load(f, encoding='bytes')\n",
    "    return dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "cifar = unpickle('./cifar-10-batches-py/data_batch_1')\n",
    "data = cifar[b'data']\n",
    "label = cifar[b'labels']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from PIL import Image\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "img = data[-1].reshape(3, 32, 32)\n",
    "red = Image.fromarray(img[0])\n",
    "green = Image.fromarray(img[1])\n",
    "blue = Image.fromarray(img[2])\n",
    "img = Image.merge(\"RGB\", (red, green, blue))\n",
    "plt.figure(\"image\")\n",
    "plt.imshow(img)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "## 数据预处理\n",
    "class CifarData():\n",
    "    def __init__(self, data, labels):\n",
    "        self._data = data\n",
    "        self._labels = labels\n",
    "        self._indicator = 0\n",
    "        self._example_num = len(data)\n",
    "        self._random_shuffle()\n",
    "        self._deal_img()\n",
    "        \n",
    "    def _random_shuffle(self):\n",
    "        p = np.random.permutation(self._example_num)\n",
    "        self._data = self._data[p]\n",
    "        self._labels = self._labels[p]\n",
    "        \n",
    "    def _deal_img(self):\n",
    "        data = self._data.reshape(self._example_num, 3, 32, 32)\n",
    "        data = data / 255\n",
    "        self._data = np.transpose(data, [0, 2, 3, 1])\n",
    "        new_label = np.zeros([self._example_num, 10])\n",
    "        for i in range(self._example_num):\n",
    "            new_label[i, self._labels[i]] = 1 \n",
    "        self._labels = new_label\n",
    "        \n",
    "    def next_batch(self, batch_size):\n",
    "        end_indicator = self._indicator + batch_size\n",
    "        if end_indicator > self._example_num:\n",
    "            self._random_shuffle()\n",
    "            self._indicator = 0\n",
    "            end_indicator = batch_size\n",
    "        batch_data = self._data[self._indicator: end_indicator]\n",
    "        batch_label = self._labels[self._indicator: end_indicator]\n",
    "        self._indicator = end_indicator        \n",
    "        return batch_data, batch_label\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0., 0., 0., 1., 0., 0., 0., 0., 0., 0.],\n",
       "       [0., 1., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "       [0., 0., 0., 1., 0., 0., 0., 0., 0., 0.],\n",
       "       [1., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0., 1., 0., 0., 0., 0., 0.],\n",
       "       [0., 0., 1., 0., 0., 0., 0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0., 0., 1., 0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0., 0., 1., 0., 0., 0., 0.],\n",
       "       [0., 1., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "       [1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cifar = unpickle('./cifar-10-batches-py/data_batch_1')\n",
    "data = cifar[b'data']\n",
    "labels = np.array(cifar[b'labels'])\n",
    "cifar_data = CifarData(data, labels)\n",
    "batch_data, batch_label = cifar_data.next_batch(10)\n",
    "batch_label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_default_parms():\n",
    "    return tf.contrib.training.HParams(\n",
    "        batch_size = 128,\n",
    "        channels = [32, 64, 128],\n",
    "        learning_rate = 0.002\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "## conv封装\n",
    "def conv2d_warpper(inputs, out_channel, name, training):\n",
    "    def leaky_relu(x, alpha=0.2, name = ''):\n",
    "        return tf.maximum(x, alpha * x, name=name)\n",
    "    with tf.variable_scope(name):\n",
    "        conv2d = tf.layers.conv2d(inputs, out_channel, [5, 5], strides=(2, 2), padding=\"SAME\")\n",
    "        bn = tf.layers.batch_normalization(conv2d, training=training)\n",
    "        return leaky_relu(bn, name='output')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NetWork():\n",
    "    \n",
    "    def __init__(self, hps):\n",
    "        self._hps = hps\n",
    "        \n",
    "    def build(self):\n",
    "        \n",
    "        self._input = tf.placeholder(tf.float32, [None, 32, 32, 3])\n",
    "        self._label = tf.placeholder(tf.int32, [None, 10])\n",
    "        \n",
    "        ## 搭建网络\n",
    "        conv_input = self._input\n",
    "        with tf.variable_scope('conv'):\n",
    "            for i in range(len(self._hps.channels)):\n",
    "                conv_input = conv2d_warpper(conv_input, self._hps.channels[i], 'conv2d_%d' % i, training=True)\n",
    "        fc_inputs = conv_input\n",
    "        with tf.variable_scope('fc'):\n",
    "            flatten = tf.layers.flatten(fc_inputs)\n",
    "            fc= tf.layers.dense(flatten, 1024, name='fc')\n",
    "        out = tf.layers.dense(fc, 10, name='output')\n",
    "        loss = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(logits=out, labels=self._label))\n",
    "        return self._input, self._label, tf.nn.softmax(out), loss\n",
    "    \n",
    "    def build_op(self, loss):\n",
    "        opt = tf.train.AdamOptimizer(learning_rate=self._hps.learning_rate)\n",
    "        return opt.minimize(loss)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(20000,)\n",
      "(30000,)\n",
      "(40000,)\n",
      "(50000,)\n",
      "step: 1 loss=13.3192425train acc:0.19140625test acc: 0.1484375\n",
      "step: 2 loss=14.528498train acc:0.23046875test acc: 0.1953125\n",
      "step: 3 loss=11.755217train acc:0.21875test acc: 0.203125\n",
      "step: 4 loss=10.617779train acc:0.19140625test acc: 0.140625\n",
      "step: 5 loss=9.04225train acc:0.1953125test acc: 0.21875\n",
      "step: 6 loss=8.166451train acc:0.18359375test acc: 0.1484375\n",
      "step: 7 loss=7.2402315train acc:0.171875test acc: 0.171875\n",
      "step: 8 loss=7.5644464train acc:0.1796875test acc: 0.171875\n",
      "step: 9 loss=5.4138765train acc:0.25390625test acc: 0.234375\n",
      "step: 10 loss=5.9129663train acc:0.234375test acc: 0.234375\n",
      "step: 11 loss=5.4094753train acc:0.23828125test acc: 0.2890625\n",
      "step: 12 loss=4.7698627train acc:0.27734375test acc: 0.2734375\n",
      "step: 13 loss=4.333523train acc:0.24609375test acc: 0.25\n",
      "step: 14 loss=4.9808016train acc:0.265625test acc: 0.2734375\n",
      "step: 15 loss=4.2850633train acc:0.2578125test acc: 0.234375\n",
      "step: 16 loss=3.8146276train acc:0.33203125test acc: 0.2734375\n",
      "step: 17 loss=3.990078train acc:0.30078125test acc: 0.3203125\n",
      "step: 18 loss=3.5089917train acc:0.28125test acc: 0.25\n",
      "step: 19 loss=3.7047112train acc:0.1796875test acc: 0.2109375\n",
      "step: 20 loss=3.361014train acc:0.3046875test acc: 0.2109375\n",
      "step: 21 loss=4.1355286train acc:0.21484375test acc: 0.203125\n",
      "step: 22 loss=3.1211948train acc:0.3515625test acc: 0.2734375\n",
      "step: 23 loss=3.7702842train acc:0.2578125test acc: 0.359375\n",
      "step: 24 loss=3.649776train acc:0.265625test acc: 0.2421875\n",
      "step: 25 loss=4.192109train acc:0.29296875test acc: 0.2421875\n",
      "step: 26 loss=3.4109073train acc:0.32421875test acc: 0.265625\n",
      "step: 27 loss=3.899021train acc:0.3125test acc: 0.34375\n",
      "step: 28 loss=4.148763train acc:0.2421875test acc: 0.3125\n",
      "step: 29 loss=2.708382train acc:0.33203125test acc: 0.3515625\n",
      "step: 30 loss=2.5896332train acc:0.2578125test acc: 0.2734375\n",
      "step: 31 loss=2.8626254train acc:0.29296875test acc: 0.3046875\n",
      "step: 32 loss=2.8859558train acc:0.28515625test acc: 0.2421875\n",
      "step: 33 loss=2.6601326train acc:0.32421875test acc: 0.359375\n",
      "step: 34 loss=2.8348308train acc:0.3125test acc: 0.359375\n"
     ]
    }
   ],
   "source": [
    "hps = get_default_parms()\n",
    "tf.reset_default_graph()\n",
    "net = NetWork(hps)\n",
    "input_ts, labels_ts, out, loss = net.build()\n",
    "opt = net.build_op(loss)\n",
    "\n",
    "## 读取训练数据\n",
    "cifar = unpickle('./cifar-10-batches-py/data_batch_1')\n",
    "all_data = cifar[b'data']\n",
    "all_label = np.array(cifar[b'labels'])\n",
    "for i in range(1, 5):\n",
    "    cifar = unpickle('./cifar-10-batches-py/data_batch_%d' % (i+1))\n",
    "    all_data = np.concatenate((all_data, cifar[b'data']), axis=0)\n",
    "    all_label = np.concatenate((all_label, np.array(cifar[b'labels'])), axis=0)\n",
    "    print(all_label.shape)\n",
    "    cifar_data = CifarData(all_data, all_label)\n",
    "\n",
    "## 读取测试数据\n",
    "cifar = unpickle('./cifar-10-batches-py/test_batch')\n",
    "test_data = cifar[b'data']\n",
    "test_label = np.array(cifar[b'labels'])\n",
    "cifar_test = CifarData(test_data, test_label)\n",
    "\n",
    "## 保存模型\n",
    "saver = tf.train.Saver()\n",
    "with tf.Session() as sess:\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    \n",
    "    step = 1001\n",
    "    for i in range(1, step):\n",
    "        batch_data, batch_label = cifar_data.next_batch(256)\n",
    "        batch_test, batch_test_label = cifar_test.next_batch(128)\n",
    "        \n",
    "        sess.run(opt, feed_dict={input_ts:batch_data, labels_ts:batch_label})\n",
    "        loss_value = sess.run(loss, feed_dict={input_ts:batch_data, labels_ts:batch_label})\n",
    "        \n",
    "\n",
    "        correct_predict = tf.equal(tf.argmax(out, 1), tf.argmax(labels_ts, 1))\n",
    "        accuracy = tf.reduce_mean(tf.cast(correct_predict, tf.float32))\n",
    "        train_acc = sess.run(accuracy, feed_dict={input_ts:batch_data, labels_ts:batch_label})\n",
    "        \n",
    "        test_acc = sess.run(accuracy, feed_dict={input_ts:batch_test, labels_ts:batch_test_label})\n",
    "        \n",
    "        print(\"step: %d\" % i + \" loss=\" + str(loss_value) + \"train acc:\" + str(train_acc)+ \\\n",
    "               \"  test acc: \" + str(test_acc))\n",
    "        \n",
    "        ### 200step 保存一下模型\n",
    "        if i % 200 == 0:\n",
    "            saver.save(sess, \"./checkpoints/myModel\")\n",
    "        "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:tf]",
   "language": "python",
   "name": "conda-env-tf-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
