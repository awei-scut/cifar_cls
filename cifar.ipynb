{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "def unpickle(file):\n",
    "    with open(file, 'rb') as f:\n",
    "        dict = pickle.load(f, encoding='bytes')\n",
    "    return dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "cifar = unpickle('./cifar-10-batches-py/data_batch_1')\n",
    "data = cifar[b'data']\n",
    "label = cifar[b'labels']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from PIL import Image\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "img = data[-1].reshape(3, 32, 32)\n",
    "red = Image.fromarray(img[0])\n",
    "green = Image.fromarray(img[1])\n",
    "blue = Image.fromarray(img[2])\n",
    "img = Image.merge(\"RGB\", (red, green, blue))\n",
    "plt.figure(\"image\")\n",
    "plt.imshow(img)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "## 数据预处理\n",
    "class CifarData():\n",
    "    def __init__(self, data, labels):\n",
    "        self._data = data\n",
    "        self._labels = labels\n",
    "        self._indicator = 0\n",
    "        self._example_num = len(data)\n",
    "        self._random_shuffle()\n",
    "        self._deal_img()\n",
    "        \n",
    "    def _random_shuffle(self):\n",
    "        p = np.random.permutation(self._example_num)\n",
    "        self._data = self._data[p]\n",
    "        self._labels = self._labels[p]\n",
    "        \n",
    "    def _deal_img(self):\n",
    "        data = self._data.reshape(self._example_num, 3, 32, 32)\n",
    "        data = data / 255\n",
    "        self._data = np.transpose(data, [0, 2, 3, 1])\n",
    "        new_label = np.zeros([self._example_num, 10])\n",
    "        for i in range(self._example_num):\n",
    "            new_label[i, self._labels[i]] = 1 \n",
    "        self._labels = new_label\n",
    "        \n",
    "    def next_batch(self, batch_size):\n",
    "        end_indicator = self._indicator + batch_size\n",
    "        if end_indicator > self._example_num:\n",
    "            self._random_shuffle()\n",
    "            self._indicator = 0\n",
    "            end_indicator = batch_size\n",
    "        batch_data = self._data[self._indicator: end_indicator]\n",
    "        batch_label = self._labels[self._indicator: end_indicator]\n",
    "        self._indicator = end_indicator        \n",
    "        return batch_data, batch_label\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0., 1., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0., 0., 0., 0., 0., 1., 0.],\n",
       "       [1., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0., 0., 0., 0., 0., 1., 0.],\n",
       "       [0., 0., 0., 0., 0., 0., 0., 0., 1., 0.],\n",
       "       [0., 0., 0., 0., 0., 0., 0., 0., 1., 0.],\n",
       "       [0., 0., 0., 0., 1., 0., 0., 0., 0., 0.],\n",
       "       [0., 0., 1., 0., 0., 0., 0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0., 1., 0., 0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0., 1., 0., 0., 0., 0., 0.]])"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cifar = unpickle('./cifar-10-batches-py/data_batch_1')\n",
    "data = cifar[b'data']\n",
    "labels = np.array(cifar[b'labels'])\n",
    "cifar_data = CifarData(data, labels)\n",
    "batch_data, batch_label = cifar_data.next_batch(10)\n",
    "batch_label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_default_parms():\n",
    "    return tf.contrib.training.HParams(\n",
    "        batch_size = 128,\n",
    "        channels = [32, 64, 128],\n",
    "        learning_rate = 0.002\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "## conv封装\n",
    "def conv2d_warpper(inputs, out_channel, name, training):\n",
    "    def leaky_relu(x, alpha=0.2, name = ''):\n",
    "        return tf.maximum(x, alpha * x, name=name)\n",
    "    with tf.variable_scope(name):\n",
    "        conv2d = tf.layers.conv2d(inputs, out_channel, [5, 5], strides=(2, 2), padding=\"SAME\")\n",
    "        bn = tf.layers.batch_normalization(conv2d, training=training)\n",
    "        return leaky_relu(bn, name='output')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NetWork():\n",
    "    \n",
    "    def __init__(self, hps):\n",
    "        self._hps = hps\n",
    "        \n",
    "    def build(self):\n",
    "        \n",
    "        self._input = tf.placeholder(tf.float32, [None, 32, 32, 3])\n",
    "        self._label = tf.placeholder(tf.int32, [None, 10])\n",
    "        \n",
    "        ## 搭建网络\n",
    "        conv_input = self._input\n",
    "        with tf.variable_scope('conv'):\n",
    "            for i in range(len(self._hps.channels)):\n",
    "                conv_input = conv2d_warpper(conv_input, self._hps.channels[i], 'conv2d_%d' % i, training=True)\n",
    "        fc_inputs = conv_input\n",
    "        with tf.variable_scope('fc'):\n",
    "            flatten = tf.layers.flatten(fc_inputs)\n",
    "            fc= tf.layers.dense(flatten, 1024, name='fc')\n",
    "        out = tf.layers.dense(fc, 10, name='output')\n",
    "        loss = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(logits=out, labels=self._label))\n",
    "        return self._input, self._label, tf.nn.softmax(out), loss\n",
    "    \n",
    "    def build_op(self, loss):\n",
    "        opt = tf.train.AdamOptimizer(learning_rate=self._hps.learning_rate)\n",
    "        return opt.minimize(loss)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Restoring parameters from ./checkpoints/myModel\n",
      "step: 1 loss=0.35854995 train_acc:0.8984375test_acc: 0.671875\n",
      "step: 2 loss=0.42117316 train_acc:0.86328125test_acc: 0.6796875\n",
      "step: 3 loss=0.48204365 train_acc:0.8359375test_acc: 0.7265625\n",
      "step: 4 loss=0.42130262 train_acc:0.875test_acc: 0.75\n",
      "step: 5 loss=0.44047892 train_acc:0.8515625test_acc: 0.7265625\n",
      "step: 6 loss=0.41938305 train_acc:0.8515625test_acc: 0.640625\n",
      "step: 7 loss=0.5212034 train_acc:0.828125test_acc: 0.671875\n",
      "step: 8 loss=0.4213887 train_acc:0.87890625test_acc: 0.71875\n",
      "step: 9 loss=0.42119008 train_acc:0.86328125test_acc: 0.71875\n",
      "step: 10 loss=0.45440602 train_acc:0.8515625test_acc: 0.6953125\n",
      "step: 11 loss=0.3733456 train_acc:0.87109375test_acc: 0.6796875\n",
      "step: 12 loss=0.43982542 train_acc:0.82421875test_acc: 0.625\n",
      "step: 13 loss=0.3566019 train_acc:0.90234375test_acc: 0.6328125\n",
      "step: 14 loss=0.3473129 train_acc:0.89453125test_acc: 0.6953125\n",
      "step: 15 loss=0.49865213 train_acc:0.82421875test_acc: 0.7578125\n",
      "step: 16 loss=0.45481157 train_acc:0.8203125test_acc: 0.6953125\n",
      "step: 17 loss=0.4456194 train_acc:0.82421875test_acc: 0.59375\n",
      "step: 18 loss=0.38782465 train_acc:0.8671875test_acc: 0.6328125\n",
      "step: 19 loss=0.425065 train_acc:0.8828125test_acc: 0.6875\n",
      "step: 20 loss=0.44222832 train_acc:0.84375test_acc: 0.6328125\n",
      "step: 21 loss=0.38388816 train_acc:0.8828125test_acc: 0.7109375\n",
      "step: 22 loss=0.43594962 train_acc:0.859375test_acc: 0.640625\n",
      "step: 23 loss=0.46175843 train_acc:0.84375test_acc: 0.6171875\n",
      "step: 24 loss=0.4885422 train_acc:0.80859375test_acc: 0.6875\n",
      "step: 25 loss=0.44494864 train_acc:0.859375test_acc: 0.640625\n",
      "step: 26 loss=0.49696422 train_acc:0.8359375test_acc: 0.671875\n",
      "step: 27 loss=0.45515913 train_acc:0.85546875test_acc: 0.6015625\n",
      "step: 28 loss=0.38816488 train_acc:0.88671875test_acc: 0.6328125\n",
      "step: 29 loss=0.44818076 train_acc:0.86328125test_acc: 0.6953125\n",
      "step: 30 loss=0.49099398 train_acc:0.8203125test_acc: 0.7265625\n",
      "step: 31 loss=0.4038438 train_acc:0.875test_acc: 0.65625\n",
      "step: 32 loss=0.45990556 train_acc:0.84765625test_acc: 0.7265625\n",
      "step: 33 loss=0.46846974 train_acc:0.828125test_acc: 0.7265625\n",
      "step: 34 loss=0.48946804 train_acc:0.85546875test_acc: 0.6640625\n",
      "step: 35 loss=0.44393438 train_acc:0.83984375test_acc: 0.6484375\n",
      "step: 36 loss=0.42958695 train_acc:0.859375test_acc: 0.65625\n",
      "step: 37 loss=0.5066453 train_acc:0.828125test_acc: 0.6875\n",
      "step: 38 loss=0.47629052 train_acc:0.84375test_acc: 0.671875\n",
      "step: 39 loss=0.45947957 train_acc:0.82421875test_acc: 0.6640625\n",
      "step: 40 loss=0.4958724 train_acc:0.84375test_acc: 0.6640625\n",
      "step: 41 loss=0.42735147 train_acc:0.83984375test_acc: 0.6953125\n",
      "step: 42 loss=0.50496894 train_acc:0.8359375test_acc: 0.71875\n",
      "step: 43 loss=0.46386993 train_acc:0.828125test_acc: 0.71875\n",
      "step: 44 loss=0.55223274 train_acc:0.81640625test_acc: 0.7265625\n",
      "step: 45 loss=0.45233408 train_acc:0.85546875test_acc: 0.703125\n",
      "step: 46 loss=0.44284475 train_acc:0.84765625test_acc: 0.671875\n",
      "step: 47 loss=0.44298834 train_acc:0.83203125test_acc: 0.6953125\n",
      "step: 48 loss=0.356834 train_acc:0.88671875test_acc: 0.65625\n",
      "step: 49 loss=0.4862767 train_acc:0.81640625test_acc: 0.6640625\n",
      "step: 50 loss=0.49012613 train_acc:0.82421875test_acc: 0.609375\n",
      "step: 51 loss=0.5090017 train_acc:0.84375test_acc: 0.6796875\n",
      "step: 52 loss=0.5140463 train_acc:0.796875test_acc: 0.6328125\n",
      "step: 53 loss=0.4579503 train_acc:0.81640625test_acc: 0.65625\n",
      "step: 54 loss=0.5342691 train_acc:0.80859375test_acc: 0.7578125\n",
      "step: 55 loss=0.43999586 train_acc:0.84765625test_acc: 0.71875\n",
      "step: 56 loss=0.4489056 train_acc:0.8515625test_acc: 0.7109375\n",
      "step: 57 loss=0.47133625 train_acc:0.80078125test_acc: 0.703125\n",
      "step: 58 loss=0.49166206 train_acc:0.8046875test_acc: 0.6171875\n",
      "step: 59 loss=0.45660204 train_acc:0.859375test_acc: 0.6171875\n",
      "step: 60 loss=0.4229688 train_acc:0.859375test_acc: 0.734375\n",
      "step: 61 loss=0.5202769 train_acc:0.828125test_acc: 0.65625\n",
      "step: 62 loss=0.47962305 train_acc:0.859375test_acc: 0.75\n",
      "step: 63 loss=0.47370693 train_acc:0.86328125test_acc: 0.7265625\n",
      "step: 64 loss=0.48105454 train_acc:0.82421875test_acc: 0.6328125\n",
      "step: 65 loss=0.4801471 train_acc:0.8359375test_acc: 0.6796875\n",
      "step: 66 loss=0.5196554 train_acc:0.82421875test_acc: 0.65625\n",
      "step: 67 loss=0.48352528 train_acc:0.83203125test_acc: 0.6875\n",
      "step: 68 loss=0.44766206 train_acc:0.82421875test_acc: 0.71875\n",
      "step: 69 loss=0.5780523 train_acc:0.7890625test_acc: 0.65625\n",
      "step: 70 loss=0.4400391 train_acc:0.83203125test_acc: 0.59375\n",
      "step: 71 loss=0.3796124 train_acc:0.875test_acc: 0.6953125\n",
      "step: 72 loss=0.5445471 train_acc:0.80859375test_acc: 0.6796875\n",
      "step: 73 loss=0.49283546 train_acc:0.8125test_acc: 0.6953125\n",
      "step: 74 loss=0.50582933 train_acc:0.8203125test_acc: 0.71875\n",
      "step: 75 loss=0.5020951 train_acc:0.84765625test_acc: 0.6875\n",
      "step: 76 loss=0.5316985 train_acc:0.8046875test_acc: 0.6640625\n",
      "step: 77 loss=0.4471476 train_acc:0.83984375test_acc: 0.6953125\n",
      "step: 78 loss=0.5005842 train_acc:0.82421875test_acc: 0.6796875\n",
      "step: 79 loss=0.6013123 train_acc:0.7890625test_acc: 0.6015625\n",
      "step: 80 loss=0.51597464 train_acc:0.828125test_acc: 0.6953125\n",
      "step: 81 loss=0.49397078 train_acc:0.81640625test_acc: 0.6875\n",
      "step: 82 loss=0.5423638 train_acc:0.80859375test_acc: 0.6875\n",
      "step: 83 loss=0.5127921 train_acc:0.8125test_acc: 0.640625\n",
      "step: 84 loss=0.53405315 train_acc:0.8125test_acc: 0.671875\n",
      "step: 85 loss=0.5557378 train_acc:0.79296875test_acc: 0.6875\n",
      "step: 86 loss=0.47244805 train_acc:0.828125test_acc: 0.65625\n",
      "step: 87 loss=0.44655764 train_acc:0.84765625test_acc: 0.640625\n",
      "step: 88 loss=0.41073814 train_acc:0.859375test_acc: 0.71875\n",
      "step: 89 loss=0.5210663 train_acc:0.8203125test_acc: 0.6484375\n",
      "step: 90 loss=0.5355049 train_acc:0.8046875test_acc: 0.625\n",
      "step: 91 loss=0.53599435 train_acc:0.8046875test_acc: 0.6640625\n",
      "step: 92 loss=0.56155837 train_acc:0.81640625test_acc: 0.6796875\n",
      "step: 93 loss=0.46456426 train_acc:0.83203125test_acc: 0.609375\n",
      "step: 94 loss=0.5398389 train_acc:0.8203125test_acc: 0.640625\n",
      "step: 95 loss=0.5690891 train_acc:0.796875test_acc: 0.609375\n",
      "step: 96 loss=0.629833 train_acc:0.78515625test_acc: 0.6328125\n",
      "step: 97 loss=0.52393526 train_acc:0.81640625test_acc: 0.7109375\n",
      "step: 98 loss=0.49228626 train_acc:0.80078125test_acc: 0.734375\n",
      "step: 99 loss=0.60101926 train_acc:0.77734375test_acc: 0.625\n",
      "step: 100 loss=0.52994525 train_acc:0.828125test_acc: 0.6796875\n",
      "step: 101 loss=0.5859341 train_acc:0.765625test_acc: 0.609375\n",
      "step: 102 loss=0.46473294 train_acc:0.84375test_acc: 0.6640625\n",
      "step: 103 loss=0.48422807 train_acc:0.8359375test_acc: 0.6796875\n",
      "step: 104 loss=0.6252309 train_acc:0.7890625test_acc: 0.7421875\n",
      "step: 105 loss=0.6169519 train_acc:0.7890625test_acc: 0.7265625\n",
      "step: 106 loss=0.5165522 train_acc:0.82421875test_acc: 0.65625\n",
      "step: 107 loss=0.55428016 train_acc:0.7890625test_acc: 0.59375\n",
      "step: 108 loss=0.5678891 train_acc:0.8125test_acc: 0.6484375\n",
      "step: 109 loss=0.4819725 train_acc:0.8359375test_acc: 0.703125\n",
      "step: 110 loss=0.60282254 train_acc:0.79296875test_acc: 0.6796875\n",
      "step: 111 loss=0.54441166 train_acc:0.80859375test_acc: 0.6796875\n",
      "step: 112 loss=0.68869007 train_acc:0.76171875test_acc: 0.625\n",
      "step: 113 loss=0.49682376 train_acc:0.828125test_acc: 0.7421875\n",
      "step: 114 loss=0.5670905 train_acc:0.80859375test_acc: 0.6640625\n",
      "step: 115 loss=0.4917582 train_acc:0.8203125test_acc: 0.609375\n",
      "step: 116 loss=0.50852984 train_acc:0.84375test_acc: 0.6953125\n",
      "step: 117 loss=0.5417806 train_acc:0.84375test_acc: 0.7265625\n",
      "step: 118 loss=0.5377753 train_acc:0.82421875test_acc: 0.71875\n",
      "step: 119 loss=0.539446 train_acc:0.8125test_acc: 0.703125\n",
      "step: 120 loss=0.5434967 train_acc:0.81640625test_acc: 0.6484375\n",
      "step: 121 loss=0.51855266 train_acc:0.82421875test_acc: 0.65625\n",
      "step: 122 loss=0.51164305 train_acc:0.82421875test_acc: 0.6640625\n",
      "step: 123 loss=0.57226944 train_acc:0.78125test_acc: 0.6640625\n",
      "step: 124 loss=0.67129874 train_acc:0.80078125test_acc: 0.5859375\n",
      "step: 125 loss=0.5523225 train_acc:0.80859375test_acc: 0.703125\n",
      "step: 126 loss=0.49632192 train_acc:0.828125test_acc: 0.6796875\n",
      "step: 127 loss=0.4880355 train_acc:0.828125test_acc: 0.6953125\n",
      "step: 128 loss=0.46217096 train_acc:0.83984375test_acc: 0.6328125\n",
      "step: 129 loss=0.5167693 train_acc:0.81640625test_acc: 0.6640625\n",
      "step: 130 loss=0.5567055 train_acc:0.796875test_acc: 0.71875\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step: 131 loss=0.54348195 train_acc:0.78125test_acc: 0.75\n",
      "step: 132 loss=0.5547216 train_acc:0.80078125test_acc: 0.6875\n",
      "step: 133 loss=0.4944804 train_acc:0.8203125test_acc: 0.7734375\n",
      "step: 134 loss=0.51389325 train_acc:0.83203125test_acc: 0.765625\n",
      "step: 135 loss=0.5015711 train_acc:0.82421875test_acc: 0.671875\n",
      "step: 136 loss=0.59536374 train_acc:0.78515625test_acc: 0.6640625\n",
      "step: 137 loss=0.52120936 train_acc:0.8203125test_acc: 0.609375\n",
      "step: 138 loss=0.5648858 train_acc:0.765625test_acc: 0.7421875\n",
      "step: 139 loss=0.50144005 train_acc:0.828125test_acc: 0.6640625\n",
      "step: 140 loss=0.45124343 train_acc:0.828125test_acc: 0.6640625\n",
      "step: 141 loss=0.51765466 train_acc:0.8046875test_acc: 0.7109375\n",
      "step: 142 loss=0.6092316 train_acc:0.7890625test_acc: 0.71875\n",
      "step: 143 loss=0.5404019 train_acc:0.78125test_acc: 0.6484375\n",
      "step: 144 loss=0.48840755 train_acc:0.8359375test_acc: 0.765625\n",
      "step: 145 loss=0.5907675 train_acc:0.78125test_acc: 0.671875\n",
      "step: 146 loss=0.54513645 train_acc:0.80078125test_acc: 0.6484375\n",
      "step: 147 loss=0.49949202 train_acc:0.828125test_acc: 0.703125\n",
      "step: 148 loss=0.4617067 train_acc:0.8515625test_acc: 0.765625\n",
      "step: 149 loss=0.5511212 train_acc:0.8046875test_acc: 0.6875\n",
      "step: 150 loss=0.5013561 train_acc:0.8125test_acc: 0.625\n",
      "step: 151 loss=0.50672495 train_acc:0.8359375test_acc: 0.609375\n",
      "step: 152 loss=0.6108916 train_acc:0.76171875test_acc: 0.7109375\n",
      "step: 153 loss=0.5526776 train_acc:0.78515625test_acc: 0.625\n",
      "step: 154 loss=0.58842814 train_acc:0.7890625test_acc: 0.6328125\n",
      "step: 155 loss=0.557009 train_acc:0.828125test_acc: 0.671875\n",
      "step: 156 loss=0.57455516 train_acc:0.79296875test_acc: 0.6875\n",
      "step: 157 loss=0.49488604 train_acc:0.84375test_acc: 0.6484375\n",
      "step: 158 loss=0.59719133 train_acc:0.8046875test_acc: 0.6640625\n",
      "step: 159 loss=0.6747355 train_acc:0.7578125test_acc: 0.6328125\n",
      "step: 160 loss=0.5879174 train_acc:0.80078125test_acc: 0.6171875\n",
      "step: 161 loss=0.5554153 train_acc:0.80859375test_acc: 0.6953125\n",
      "step: 162 loss=0.5493884 train_acc:0.8125test_acc: 0.6875\n",
      "step: 163 loss=0.5397849 train_acc:0.8203125test_acc: 0.6875\n",
      "step: 164 loss=0.72847235 train_acc:0.734375test_acc: 0.671875\n",
      "step: 165 loss=0.53793204 train_acc:0.8125test_acc: 0.71875\n",
      "step: 166 loss=0.5762346 train_acc:0.796875test_acc: 0.6796875\n",
      "step: 167 loss=0.5604737 train_acc:0.78125test_acc: 0.6796875\n",
      "step: 168 loss=0.4682916 train_acc:0.83203125test_acc: 0.6796875\n",
      "step: 169 loss=0.54571694 train_acc:0.80859375test_acc: 0.6328125\n",
      "step: 170 loss=0.5220232 train_acc:0.80078125test_acc: 0.6875\n",
      "step: 171 loss=0.60937065 train_acc:0.78125test_acc: 0.65625\n",
      "step: 172 loss=0.72798836 train_acc:0.7421875test_acc: 0.6796875\n",
      "step: 173 loss=0.5718951 train_acc:0.80078125test_acc: 0.6953125\n",
      "step: 174 loss=0.6200447 train_acc:0.76953125test_acc: 0.6484375\n",
      "step: 175 loss=0.43980712 train_acc:0.86328125test_acc: 0.6484375\n",
      "step: 176 loss=0.5862256 train_acc:0.80859375test_acc: 0.765625\n",
      "step: 177 loss=0.5921445 train_acc:0.80859375test_acc: 0.65625\n",
      "step: 178 loss=0.6435032 train_acc:0.78125test_acc: 0.7109375\n",
      "step: 179 loss=0.51439583 train_acc:0.82421875test_acc: 0.75\n",
      "step: 180 loss=0.5840633 train_acc:0.78125test_acc: 0.703125\n",
      "step: 181 loss=0.47250187 train_acc:0.83984375test_acc: 0.671875\n",
      "step: 182 loss=0.4029557 train_acc:0.86328125test_acc: 0.671875\n",
      "step: 183 loss=0.5649746 train_acc:0.796875test_acc: 0.6953125\n",
      "step: 184 loss=0.4048795 train_acc:0.8515625test_acc: 0.65625\n",
      "step: 185 loss=0.5576588 train_acc:0.81640625test_acc: 0.6875\n",
      "step: 186 loss=0.53100336 train_acc:0.81640625test_acc: 0.6875\n",
      "step: 187 loss=0.46987072 train_acc:0.86328125test_acc: 0.7109375\n",
      "step: 188 loss=0.49241576 train_acc:0.81640625test_acc: 0.7578125\n",
      "step: 189 loss=0.5055231 train_acc:0.8203125test_acc: 0.7109375\n",
      "step: 190 loss=0.6441308 train_acc:0.76171875test_acc: 0.6953125\n",
      "step: 191 loss=0.54840463 train_acc:0.8125test_acc: 0.7265625\n",
      "step: 192 loss=0.5217544 train_acc:0.8203125test_acc: 0.703125\n",
      "step: 193 loss=0.57837427 train_acc:0.79296875test_acc: 0.703125\n",
      "step: 194 loss=0.5309726 train_acc:0.81640625test_acc: 0.7265625\n",
      "step: 195 loss=0.4602652 train_acc:0.83984375test_acc: 0.671875\n",
      "step: 196 loss=0.40202755 train_acc:0.83984375test_acc: 0.6953125\n",
      "step: 197 loss=0.42343163 train_acc:0.859375test_acc: 0.703125\n",
      "step: 198 loss=0.42774874 train_acc:0.86328125test_acc: 0.6953125\n",
      "step: 199 loss=0.37004906 train_acc:0.8828125test_acc: 0.7109375\n"
     ]
    }
   ],
   "source": [
    "hps = get_default_parms()\n",
    "tf.reset_default_graph()\n",
    "net = NetWork(hps)\n",
    "input_ts, labels_ts, out, loss = net.build()\n",
    "opt = net.build_op(loss)\n",
    "\n",
    "## 读取训练数据\n",
    "cifar = unpickle('./cifar-10-batches-py/data_batch_1')\n",
    "all_data = cifar[b'data']\n",
    "all_label = np.array(cifar[b'labels'])\n",
    "for i in range(1, 5):\n",
    "    cifar = unpickle('./cifar-10-batches-py/data_batch_%d' % (i+1))\n",
    "    all_data = np.concatenate((all_data, cifar[b'data']), axis=0)\n",
    "    all_label = np.concatenate((all_label, np.array(cifar[b'labels'])), axis=0)\n",
    "    cifar_data = CifarData(all_data, all_label)\n",
    "\n",
    "## 读取测试数据\n",
    "cifar = unpickle('./cifar-10-batches-py/test_batch')\n",
    "test_data = cifar[b'data']\n",
    "test_label = np.array(cifar[b'labels'])\n",
    "cifar_test = CifarData(test_data, test_label)\n",
    "\n",
    "## 保存模型\n",
    "saver = tf.train.Saver()\n",
    "with tf.Session() as sess:\n",
    "    \n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    saver.restore(sess, './checkpoints/myModel')\n",
    "    step = 1001\n",
    "    for i in range(1, step):\n",
    "        batch_data, batch_label = cifar_data.next_batch(256)\n",
    "        batch_test, batch_test_label = cifar_test.next_batch(128)\n",
    "        \n",
    "        sess.run(opt, feed_dict={input_ts:batch_data, labels_ts:batch_label})\n",
    "        loss_value = sess.run(loss, feed_dict={input_ts:batch_data, labels_ts:batch_label})\n",
    "        \n",
    "\n",
    "        correct_predict = tf.equal(tf.argmax(out, 1), tf.argmax(labels_ts, 1))\n",
    "        accuracy = tf.reduce_mean(tf.cast(correct_predict, tf.float32))\n",
    "        train_acc = sess.run(accuracy, feed_dict={input_ts:batch_data, labels_ts:batch_label})\n",
    "        \n",
    "        test_acc = sess.run(accuracy, feed_dict={input_ts:batch_test, labels_ts:batch_test_label})\n",
    "        \n",
    "        print(\"step: %d\" % i + \" loss=\" + str(loss_value) + \" train_acc:\" + str(train_acc)+ \\\n",
    "               \"test_acc: \" + str(test_acc))\n",
    "        \n",
    "        ### 200step 保存一下模型\n",
    "        if i % 200 == 0:\n",
    "            saver.save(sess, \"./checkpoints/myModel\")\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:tf]",
   "language": "python",
   "name": "conda-env-tf-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
